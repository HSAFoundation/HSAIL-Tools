// University of Illinois/NCSA
// Open Source License
// 
// Copyright (c) 2013, Advanced Micro Devices, Inc.
// All rights reserved.
// 
// Developed by:
// 
//     HSA Team
// 
//     Advanced Micro Devices, Inc
// 
//     www.amd.com
// 
// Permission is hereby granted, free of charge, to any person obtaining a copy of
// this software and associated documentation files (the "Software"), to deal with
// the Software without restriction, including without limitation the rights to
// use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies
// of the Software, and to permit persons to whom the Software is furnished to do
// so, subject to the following conditions:
// 
//     * Redistributions of source code must retain the above copyright notice,
//       this list of conditions and the following disclaimers.
// 
//     * Redistributions in binary form must reproduce the above copyright notice,
//       this list of conditions and the following disclaimers in the
//       documentation and/or other materials provided with the distribution.
// 
//     * Neither the names of the LLVM Team, University of Illinois at
//       Urbana-Champaign, nor the names of its contributors may be used to
//       endorse or promote products derived from this Software without specific
//       prior written permission.
// 
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS
// FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
// CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE
// SOFTWARE.
version 1:0:$full:$large;

function &TestFunc()()
{
	ret;
};

function &TestFunc01()(arg_u8x8 %in_arg0)
{
@lab1:
	ret;
};

function &TestFunc12(arg_s32x2 %out_arg0)(
	arg_u32 %in_arg0,
	arg_u32 %in_arg1)
{
@lab1:
	ret;
};

function &TestCalls()()
{
@lab1:
	{
		call	&TestFunc () ();
	}
	{
		call	&TestFunc ();
	}
	{
		call	&TestFunc;
	}
	{
		call	$d0;
	}
	{
		call_width(2147483648)	$d0;
	}
	{
		call_width(all)	$d0;
	}
	{
		call_width(WAVESIZE)	$d0;
	}
	{
		arg_u8x8 %iarg0;

		st_arg_u64	0, [%iarg0];
		call	&TestFunc01 () (%iarg0);
	}
	{
		arg_u32 %iarg0;
		arg_u32 %iarg1;

		st_arg_u32	0, [%iarg0];
		st_arg_u32	0, [%iarg1];
		call	&TestFunc12 () (%iarg0, %iarg1);
	}
	{
		arg_u32 %iarg0;
		arg_u32 %iarg1;

		st_arg_u32	0, [%iarg0];
		call	&TestFunc12 () (%iarg0, %iarg0);
	}
	{
		arg_s32x2 %oarg;
		arg_u32 %iarg0;
		arg_u32 %iarg1;

		st_arg_u32	0, [%iarg0];
		st_arg_u32	0, [%iarg1];
		call	&TestFunc12 (%oarg) (%iarg0, %iarg1);
		ld_arg_u64    $d0, [%oarg];
	}
	ret;
};

function &TestCalls9()()
{
	{
		arg_u32 %iarg0;
		arg_u32 %iarg1;

		st_arg_u32	0, [%iarg0];
		call	&TestFunc12 () (%iarg0, %iarg0);
	}
};

global_u32 &x;
group_u32 &g;

global_roimg &roimage = {width = 5, height = 4, depth = 6, format = unorm_short_101010, order = rgbx};
global_rwimg &rwimage = {width = 5, height = 4, depth = 6, format = unorm_short_101010, order = rgbx};
global_samp  &samp    = {coord = normalized, filter = nearest, boundaryU = clamp, boundaryV = clamp, boundaryW = clamp};

kernel &TestKernel(kernarg_u32 %arg)
{
    lda_kernarg_u64   $d4, [%arg];
};

function &Testinst()(arg_u32 %arg)
{
    abs_s32             $s1, $s2;
    abs_s64             $d1, $d2;

    abs_p_s8x4          $s1, $s2;
    abs_s_s32x2         $d1, $d1;

    abs_s_s8x4           $s1, $s2;
    abs_s_s16x2          $s1, $s2;
    abs_s_s8x8          $d1, $d1;
    abs_s_s16x4         $d1, $d1;
    abs_s_s32x2         $d1, $d1;
    abs_s_s8x16         $q1, $q1;
    abs_s_s16x8         $q1, $q1;
    abs_s_s32x4         $q1, $q1;
    abs_s_s64x2         $q1, $q1;

    abs_p_f16x2         $s1, $s1;
    abs_s_f16x4         $d1, $d1;
    abs_s_f32x2         $d1, $d1;
    abs_s_f16x8         $q1, $q1;
    abs_s_f32x4         $q1, $q1;
    abs_s_f64x2         $q1, $q1;

    abs_f16             $s1,$s2;
    abs_f32             $s1,$s2; 
    abs_f64             $d1,$d2;

    //---------------------------------------

    neg_s32             $s1, 100;
    neg_s64             $d1, $d2;

    neg_s_s8x4          $s1, $s2;
    neg_p_s8x4      $s1, $s2;

    neg_f32             $s3,1.0f;
    neg_f64             $d3,1.0;

    //---------------------------------------

    add_s32             $s1, 42, $s2;
    add_u32             $s1, $s2, 0x23;
    add_s64             $d1, $d2, 23;
    add_u64             $d1, 61, 0x233412349456;

    add_pp_sat_u16x2    $s1, $s0, $s3;
    add_pp_u16x4        $d1, $d0, $d3;

    add_ftz_f16             $s3,$s2,$s1;
    add_ftz_f32             $s3,$s2,$s1;
    add_ftz_f64             $d3,$d2,$d1;

    add_up_f16             $s3,$s2,$s1;
    add_down_f32             $s3,$s2,$s1;
    add_zero_f64             $d3,$d2,$d1;
    add_near_f64             $d3,$d2,$d1;

    add_ftz_near_f16             $s3,$s2,$s1;
    add_ftz_up_f32             $s3,$s2,$s1;
    add_ftz_down_f32             $s3,$s2,$s1;
    add_ftz_zero_f32             $s3,$s2,$s1;




    add_ftz_pp_f16x2             $s3,$s2,$s1;
    add_ftz_ps_f16x4             $d3,$d2,$d1;
    add_ftz_sp_f32x2             $d3,$d2,$d1;
    add_ftz_ss_f16x8             $q3,$q2,$q1;
    add_ftz_pp_f32x4             $q3,$q2,$q1;
    add_ftz_ps_f64x2             $q3,$q2,$q1;

    add_up_pp_f16x2             $s3,$s2,$s1;
    add_up_ps_f16x4             $d3,$d2,$d1;
    add_up_sp_f32x2             $d3,$d2,$d1;
    add_up_ss_f16x8             $q3,$q2,$q1;
    add_up_pp_f32x4             $q3,$q2,$q1;
    add_up_ps_f64x2             $q3,$q2,$q1;

    add_down_pp_f16x2             $s3,$s2,$s1;
    add_down_ps_f16x4             $d3,$d2,$d1;
    add_down_sp_f32x2             $d3,$d2,$d1;
    add_down_ss_f16x8             $q3,$q2,$q1;
    add_down_pp_f32x4             $q3,$q2,$q1;
    add_down_ps_f64x2             $q3,$q2,$q1;

    add_zero_pp_f16x2             $s3,$s2,$s1;
    add_zero_ps_f16x4             $d3,$d2,$d1;
    add_zero_sp_f32x2             $d3,$d2,$d1;
    add_zero_ss_f16x8             $q3,$q2,$q1;
    add_zero_pp_f32x4             $q3,$q2,$q1;
    add_zero_ps_f64x2             $q3,$q2,$q1;

    add_near_pp_f16x2             $s3,$s2,$s1;
    add_near_ps_f16x4             $d3,$d2,$d1;
    add_near_sp_f32x2             $d3,$d2,$d1;
    add_near_ss_f16x8             $q3,$q2,$q1;
    add_near_pp_f32x4             $q3,$q2,$q1;
    add_near_ps_f64x2             $q3,$q2,$q1;

    add_ftz_near_pp_f16x2             $s3,$s2,$s1;
    add_ftz_near_ps_f16x4             $d3,$d2,$d1;
    add_ftz_near_sp_f32x2             $d3,$d2,$d1;
    add_ftz_near_ss_f16x8             $q3,$q2,$q1;
    add_ftz_near_pp_f32x4             $q3,$q2,$q1;
    add_ftz_near_ps_f64x2             $q3,$q2,$q1;

    add_ftz_up_pp_f16x2             $s3,$s2,$s1;
    add_ftz_up_ps_f16x4             $d3,$d2,$d1;
    add_ftz_up_sp_f32x2             $d3,$d2,$d1;
    add_ftz_up_ss_f16x8             $q3,$q2,$q1;
    add_ftz_up_pp_f32x4             $q3,$q2,$q1;
    add_ftz_up_ps_f64x2             $q3,$q2,$q1;

    add_ftz_down_pp_f16x2             $s3,$s2,$s1;
    add_ftz_down_ps_f16x4             $d3,$d2,$d1;
    add_ftz_down_sp_f32x2             $d3,$d2,$d1;
    add_ftz_down_ss_f16x8             $q3,$q2,$q1;
    add_ftz_down_pp_f32x4             $q3,$q2,$q1;
    add_ftz_down_ps_f64x2             $q3,$q2,$q1;

    add_ftz_zero_pp_f16x2            $s3,$s2,$s1;
    add_ftz_zero_ps_f16x4            $d3,$d2,$d1;
    add_ftz_zero_sp_f32x2            $d3,$d2,$d1;
    add_ftz_zero_ss_f16x8            $q3,$q2,$q1;
    add_ftz_zero_pp_f32x4            $q3,$q2,$q1;
    add_ftz_zero_ps_f64x2            $q3,$q2,$q1;
    //---------------------------------------

    div_s32             $s1, 100, 10;
    div_u32             $s1, $s2, 0x23;
    div_s64             $d1, $d2, 23;
    div_u64             $d1, $d3, 0x233412349456;

    //---------------------------------------

    max_s32             $s1, 100, 10;
    max_u32             $s1, $s2, 0x23;
    max_s64             $d1, $d2, 23;
    max_u64             $d1, $d3, 0x233412349456;

    //---------------------------------------

    min_s32             $s1, 100, 10;
    min_u32             $s1, $s2, 0x23;
    min_s64             $d1, $d2, 23;
    min_u64             $d1, $d3, 0x233412349456;

    //---------------------------------------

    mul_s32             $s1, 100, 10;
    mul_u32             $s1, $s2, 0x23;
    mul_s64             $d1, $d2, 23;
    mul_u64             $d1, $d3, 0x233412349456;

    //---------------------------------------

    mulhi_s32           $s1, $s3, $s3;
    mulhi_u32           $s1, $s2, $s9;

    //---------------------------------------

    rem_s32             $s1, 100, 10;
    rem_u32             $s1, $s2, 0xFFFFFFFF;
    rem_s64             $d1, $d2, 23;
    rem_u64             $d1, $d3, 0x233412349456;

    //---------------------------------------

    sub_s32             $s1, 100, 10;
    sub_u32             $s1, $s2, 0x23;
    sub_s64             $d1, $d2, 23;
    sub_u64             $d1, $d3, 0x233412349456;

    //---------------------------------------

    mul_pp_u16x4        $d1, $d0, $d3;

    mulhi_pp_u8x8       $d1, $d3, $d4;

    sub_sp_u8x8         $d1, $d0, $d3;
    max_pp_u8x4         $s1, $s0, $s3;
    min_pp_u8x4         $s1, $s0, $s3;

    //---------------------------------------

    mad_s32             $s1, $s2, $s3, $s5;
    mad_s64             $d1, $d2, $d3, $d2;
    mad_u32             $s1, $s2, $s3, $s3;
    mad_u64             $d1, $d2, $d3, $d1;

    //---------------------------------------

    mad24_s32           $s1, $s2, -12, 23;
    mad24_u32           $s1, $s2, 12, 2;
    mad24hi_s32         $s1, $s2, -12, 23;
    mad24hi_u32         $s1, $s2, 12, 2;
    mul24_s32           $s1, $s2, -12;
    mul24_u32           $s1, $s2, 12;
    mul24hi_s32         $s1, $s2, -12;
    mul24hi_u32         $s1, $s2, 12;

    //---------------------------------------

    shl_u32             $s1, $s2, 2;
    shl_u64             $d1, $d2, 2;
    shl_s32             $s1, $s2, 2;
    shl_s64             $d1, $d2, 2;
    shr_u32             $s1, $s2, 2;
    shr_u64             $d1, $d2, 2;
    shr_s32             $s1, $s2, 2;
    shr_s64             $d1, $d2, 2;
    shl_u8x8            $d0, $d1, 2;
    shl_u8x4            $s1, $s2, 2;
    shl_u8x8            $d1, $d2, 1;
    shr_u8x4            $s1, $s2, 1;
    shr_u8x8            $d1, $d2, 2;

    //---------------------------------------

    and_b1              $c0, $c2, $c3;
    and_b32             $s0, $s2, $s3;
    and_b64             $d0, $d1, $d2;
    or_b1               $c0, $c2, $c3;
    or_b32              $s0, $s2, $s3;
    or_b64              $d0, $d1, $d2;
    xor_b1              $c0, $c2, $c3;
    xor_b32             $s0, $s2, $s3;
    xor_b64             $d0, $d1, $d2;
    not_b1              $c1, $c2;
    not_b32             $s0, $s2;
    not_b64             $d0, $d1;

    //---------------------------------------

    popcount_u32_b32    $s1, $s2;
    popcount_u32_b64    $s1, $d2;

    popcount_u32_b32    $s1, 0xFFFFFFFF;
    popcount_u32_b64    $s1, 0xFFFFFFFFFFFFFFFF;
    //---------------------------------------

    bitrev_b32          $s1, $s2;
    bitrev_b64          $d1, 0x234;

    bitextract_s32      $s1, $s1, 2, 3;
    bitextract_s32      $s1, $s1, $s0, $s0;
    bitextract_u64      $d1, $d1, $s1, $s2;

    bitinsert_s32       $s1, $s1, $s2, 2, 3;
    bitinsert_s32       $s1, $s1, $s2, $s0, $s0;
    bitinsert_u32       $s1, $s1, $s2, $s0, $s0;
    bitinsert_u64       $d1, $d2, $d3, $s1, $s2;

    bitmask_b32         $s0, $s1, $s2;
    bitmask_b64         $d0, $s1, $s2;
    bitmask_b32         $s0, 1, 0;
    bitmask_b64         $d0, 1, 2;

    bitselect_b32       $s3, $s0, $s3, $s4;

    firstbit_u32_s32    $s0, $s0;
    firstbit_u32_u64    $s0, $d6;
    firstbit_u32_s32    $s0, 0xFFFFFFFF;
    firstbit_u32_u64    $s0, 0xFFFFFFFFFFFFFFFF;

    lastbit_u32_u32     $s0, $s0;
    lastbit_u32_s64     $s0, $d6;
    lastbit_u32_u32     $s0, 0xFFFFFFFF;
    lastbit_u32_s64     $s0, 0xFFFFFFFFFFFFFFFF;

    //---------------------------------------

    combine_v2_b64_b32      $d0,                    ($s0, $s1);
    combine_v4_b128_b32     $q0,                    ($s0, $s1, $s2, $s3);
    combine_v2_b128_b64     $q0,                    ($d0, $d1);

    //---------------------------------------

    expand_v2_b32_b64       ($s0, $s1),             $d0;
    expand_v4_b32_b128      ($s0, $s1, $s2, $s3),   $q0;
    expand_v2_b64_b128      ($d0, $d1),             $q0;

    expand_v2_b32_b64       ($s0, $s1),             0xFFFFFFFFFFFFFFFF;

    //---------------------------------------

    mov_b1              $c1, 0;
    mov_b1              $c1, $c2;
    mov_b32             $s1, 0;
    mov_b32             $s1, 0.0f;
    mov_b32             $s1, $s7;
    mov_b64             $d1, 0;
    mov_b64             $d1, 0.0;
    mov_b64             $d1, $d8;
    mov_b128            $q1, $q2;

    mov_f16             $s1, $s8;

    mov_s32             $s1, $s8;
    mov_u32             $s1, $s8;
    mov_f32             $s1, $s8;

    mov_s64             $d1, $d8;
    mov_u64             $d1, $d8;
    mov_f64             $d1, $d8;
    mov_samp            $d1, $d8;
    mov_roimg           $d1, $d8;
    mov_rwimg           $d1, $d8;

    //---------------------------------------

    global_b8   %globalVar[8];   lda_global_u64   $d4, [%globalVar];
    group_b8    %groupVar[8];    lda_group_u32    $s4, [%groupVar];
    private_b8  %privateVar[8];  lda_private_u32  $s4, [%privateVar];
    readonly_b8 %readonlyVar[8]; lda_readonly_u64 $d4, [%readonlyVar];
    spill_b8    %spillVar[8];    lda_spill_u32    $s4, [%spillVar];
                                 lda_arg_u32      $s4, [%arg];

    group_b8 %gs[8];
    lda_group_u32 $s4, [%gs];
    lda_group_u32 $s4, [%gs][12];
    lda_group_u32 $s4, [%gs][$s0 + 12];
    lda_group_u32 $s4, [$s0 + 12];
    lda_group_u32 $s4, [$s0];

    global_u32 %g[3];
    lda_global_u64 $d1, [%g];
    lda_global_u64 $d1, [$d7 + 4];

    stof_global_u64_u64     $d0, $d1;
    lda_global_u64      $d1, [$d1 + 8];
    lda_global_u64      $d1, [800];
    lda_global_u64      $d1, [$d1];

    //---------------------------------------

@lab:
    ldc_u64 $d1, &TestFunc;
    ldc_u64 $d2, @lab;


    //---------------------------------------

    shuffle_u8x4        $s10, $s12, $s12, 0x55;


    //---------------------------------------

    unpacklo_u8x4       $s1, $s2, 72;
    unpackhi_f16x2      $s3, $s3,$s4;

   
    //---------------------------------------

    pack_f16x2_f16      $s1, $s2, $s3, $s1;
    pack_f16x4_f16      $d1, $d2, $s3, $s3;
    pack_f16x8_f16      $q1, $q2, $s3, $s3;

    pack_f32x2_f32      $d1, $d1, $s2, $s0;
    pack_f32x4_f32      $q1, $q1, $s2, $s0;

    pack_f64x2_f64      $q1, $q1, $d2, $s0;

    pack_u8x4_u32       $s1, $s2, $s3, $s0;
    pack_u8x8_u32       $d1, $d2, $s3, $s0;
    pack_u8x16_u32      $q1, $q2, $s3, $s0;
    pack_u16x2_u32      $s1, $s2, $s3, $s0;
    pack_u16x4_u32      $d1, $d2, $s3, $s0;
    pack_u16x8_u32      $q1, $q2, $s3, $s0;
    pack_u32x2_u32      $d1, $d1, $s1, $s0;
    pack_u32x4_u32      $q1, $q1, $s1, $s0;

    pack_u8x4_u64       $s1, $s2, $d3, $s0;
    pack_u8x8_u64       $d1, $d2, $d3, $s0;
    pack_u8x16_u64      $q1, $q2, $d3, $s0;
    pack_u16x2_u64      $s1, $s2, $d3, $s0;
    pack_u16x4_u64      $d1, $d2, $d3, $s0;
    pack_u16x8_u64      $q1, $q2, $d3, $s0;
    pack_u32x2_u64      $d1, $d1, $d1, $s0;
    pack_u32x4_u64      $q1, $q1, $d1, $s0;
    pack_u64x2_u64      $q1, $q1, $d1, $s0;

    pack_s8x4_s32       $s1, $s2, $s3, $s0;
    pack_s8x8_s32       $d1, $d2, $s3, $s0;
    pack_s8x16_s32      $q1, $q2, $s3, $s0;
    pack_s16x2_s32      $s1, $s2, $s3, $s0;
    pack_s16x4_s32      $d1, $d2, $s3, $s0;
    pack_s16x8_s32      $q1, $q2, $s3, $s0;
    pack_s32x2_s32      $d1, $d1, $s1, $s0;
    pack_s32x4_s32      $q1, $q1, $s1, $s0;

    pack_s8x4_s64       $s1, $s2, $d3, $s0;
    pack_s8x8_s64       $d1, $d2, $d3, $s0;
    pack_s8x16_s64      $q1, $q2, $d3, $s0;
    pack_s16x2_s64      $s1, $s2, $d3, $s0;
    pack_s16x4_s64      $d1, $d2, $d3, $s0;
    pack_s16x8_s64      $q1, $q2, $d3, $s0;
    pack_s32x2_s64      $d1, $d1, $d1, $s0;
    pack_s32x4_s64      $q1, $q1, $d1, $s0;
    pack_s64x2_s64      $q1, $q1, $d1, $s0;


    pack_s64x2_s64      $q1, $q1, $d1, $s0;
    pack_s16x4_s64      $d1, $d1, $d2, $s1;
    pack_u32x2_u64      $d1, $d2, $d3, $s3;

    pack_u32x2_u32      $d1, $d1, 0xFFFFFFFF, 0xFFFFFFFF;
    pack_s64x2_s64      $q1, $q1, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFF;

    pack_f32x2_f32      $d1, $d1, $s2, 1;
    pack_f32x4_f32      $q1, $q1, $s2, 3;
    pack_u32x2_u32      $d1, $d1, $s1, 2;
    pack_s64x2_s64      $q1, $q1, $d1, 0;

    pack_u8x4_u32       $s1, $s2, $s3, 2;
    pack_s16x4_s64      $d1, $d1, $d2, 1;
    pack_u32x2_u64      $d1, $d2, $d3, 3;
    pack_f16x2_f16      $s1, $s2, $s3, 1;
    pack_f16x4_f16      $d1, $d2, $s3, 3;
    
    //---------------------------------------

    unpack_f16_f16x2    $s1, $s2, $s1;
    unpack_f16_f16x4    $s1, $d2, $s3;
    unpack_f16_f16x8    $s1, $q2, $s3;
    unpack_f32_f32x2    $s1, $d2, $s1;
    unpack_f32_f32x4    $s1, $q2, $s3;
    unpack_f64_f64x2    $d1, $q2, $s3;

    unpack_u32_u32x2    $s1, $d1, $s2;
    unpack_u32_u32x4    $s1, $q1, $s2;
    unpack_u32_u16x2    $s1, $s1, $s2;
    unpack_u32_u16x4    $s1, $d1, $s2;
    unpack_u32_u16x8    $s1, $q1, $s2;
    unpack_u32_u8x4     $s1, $s1, $s2;
    unpack_u32_u8x8     $s1, $d1, $s2;
    unpack_u32_u8x16    $s1, $q1, $s2;
    unpack_u64_u64x2    $d1, $q1, $s2;
    unpack_u64_u32x2    $d1, $d1, $s2;
    unpack_u64_u32x4    $d1, $q1, $s2;
    unpack_u64_u16x2    $d1, $s1, $s2;
    unpack_u64_u16x4    $d1, $d1, $s2;
    unpack_u64_u16x8    $d1, $q1, $s2;
    unpack_u64_u8x4     $d1, $s1, $s2;
    unpack_u64_u8x8     $d1, $d1, $s2;
    unpack_u64_u8x16    $d1, $q1, $s2;

    unpack_s32_s32x2    $s1, $d1, $s2;
    unpack_s32_s32x4    $s1, $q1, $s2;
    unpack_s32_s16x2    $s1, $s1, $s2;
    unpack_s32_s16x4    $s1, $d1, $s2;
    unpack_s32_s16x8    $s1, $q1, $s2;
    unpack_s32_s8x4     $s1, $s1, $s2;
    unpack_s32_s8x8     $s1, $d1, $s2;
    unpack_s32_s8x16    $s1, $q1, $s2;
    unpack_s64_s64x2    $d1, $q1, $s2;
    unpack_s64_s32x2    $d1, $d1, $s2;
    unpack_s64_s32x4    $d1, $q1, $s2;
    unpack_s64_s16x2    $d1, $s1, $s2;
    unpack_s64_s16x4    $d1, $d1, $s2;
    unpack_s64_s16x8    $d1, $q1, $s2;
    unpack_s64_s8x4     $d1, $s1, $s2;
    unpack_s64_s8x8     $d1, $d1, $s2;
    unpack_s64_s8x16    $d1, $q1, $s2;


    unpack_s64_s64x2    $d1, $q1, $s0;

    unpack_u32_u8x4     $s1, $s2, $s2;
    unpack_s32_s16x4    $s1, $d1, $s0;
    unpack_u64_u32x4    $d1, $q1, $s2;
    unpack_s64_s32x2    $d1, $d2, $s0;

    unpack_u32_u8x4     $s1, $s2, $s0;
    unpack_u32_u8x4     $s1, $s2, $s1;
    unpack_u32_u8x4     $s1, $s2, $s2;
    unpack_u32_u8x4     $s1, $s2, $s3;

    unpack_f32_f32x2    $s1, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFF;

    unpack_f32_f32x2    $s1, $d2, 1;
    unpack_u32_u8x4     $s1, $s2, 2;
    unpack_s32_s16x4    $s1, $d1, 0;
    unpack_u64_u32x4    $d1, $q1, 2;
    unpack_s64_s32x2    $d1, $d2, 0;
    unpack_f16_f16x2    $s1, $s2, 1;
    unpack_f16_f16x4    $s1, $d2, 3;
    unpack_f32_f32x4    $s1, $q2, 3;
    unpack_u32_u32x4    $s1, $q1, 2;
    unpack_s64_s64x2    $d1, $q1, 0;

    //---------------------------------------

    cmov_b32            $s1, $c3, $s1, $s2;
    cmov_b32            $s1, 1, $s1, $s2;
    cmov_b64            $d1, $c3, $d1, $d2;
    cmov_b32            $s1, $c0, $s1, $s2;
    cmov_u8x4           $s1, $s0, $s1, $s2;
    cmov_s8x4           $s1, $s0, $s1, $s2;
    cmov_s8x8           $d1, $d0, $d1, $d2;
    cmov_s64x2          $q1, $q0, $q1, $q2;

    copysign_f32        $s3,$s2,$s1;
    copysign_f64        $d3,$d2,$d1;
    div_f32             $s3,1.0f,$s1;
    div_f64             $d3,1.0,$d0;
    fma_f32             $s3,1.0f,$s1,23.f;
    fma_f64             $d3,1.0,$d0, $d3;
    max_f32             $s3,1.0f,$s1;
    max_f64             $d3,1.0,$d0;
    min_f32             $s3,1.0f,$s1;
    min_f64             $d3,1.0,$d0;
    mul_f32             $s3,1.0f,$s1;
    mul_f64             $d3,1.0,$d0;

    sub_f32             $s3,1.0f,$s1;
    sub_f64             $d3,1.0,$d0;
    fract_f32           $s0, 3.2f;

    //---------------------------------------

    sqrt_f16           $s0, 3.2f;
    sqrt_f32           $s0, 3.2f;
    sqrt_f64           $d0, 3.2;
    sqrt_ftz_f16           $s0, 3.2f;
    sqrt_near_f32           $s0, 3.2f;

    sqrt_p_f16x2           $s0, $s0;
    sqrt_s_f32x2           $d0, $d0;
    sqrt_p_f64x2           $q0, $q0;
    sqrt_ftz_s_f16x2       $s0, $s0;
    sqrt_near_p_f32x2      $d0, $d0;
    sqrt_zero_p_f32x2      $d0, $d0;

    //---------------------------------------

    ceil_f16           $s0, 3.2f;
    ceil_f32           $s0, 3.2f;
    ceil_f64           $d0, 3.2;
    ceil_ftz_f16           $s0, 3.2f;
    ceil_p_f16x2           $s0, $s0;
    ceil_s_f32x2           $d0, $d0;
    ceil_p_f64x2           $q0, $q0;
    ceil_ftz_s_f16x2       $s0, $s0;

    //---------------------------------------

    floor_f16           $s0, 3.2f;
    floor_f32           $s0, 3.2f;
    floor_f64           $d0, 3.2;
    floor_ftz_f16           $s0, 3.2f;
    floor_p_f16x2           $s0, $s0;
    floor_s_f32x2           $d0, $d0;
    floor_p_f64x2           $q0, $q0;
    floor_ftz_s_f16x2       $s0, $s0;

    //---------------------------------------

    rint_f16           $s0, 3.2f;
    rint_f32           $s0, 3.2f;
    rint_f64           $d0, 3.2;
    rint_ftz_f16           $s0, 3.2f;
    rint_p_f16x2           $s0, $s0;
    rint_s_f32x2           $d0, $d0;
    rint_p_f64x2           $q0, $q0;
    rint_ftz_s_f16x2       $s0, $s0;

    //---------------------------------------

    trunc_f16           $s0, 3.2f;
    trunc_f32           $s0, 3.2f;
    trunc_f64           $d0, 3.2;
    trunc_ftz_f16           $s0, 3.2f;
    trunc_p_f16x2           $s0, $s0;
    trunc_s_f32x2           $d0, $d0;
    trunc_p_f64x2           $q0, $q0;
    trunc_ftz_s_f16x2       $s0, $s0;

    //---------------------------------------

    class_b1_f16        $c1, $s1, $s0;
    class_b1_f32        $c1, $s1, 3;
    class_b1_f32        $c1, 1.0f, 3;
    class_b1_f32        $c1, $s1, $s2;
    class_b1_f64        $c1, 1.0, $s2;
    class_b1_f64        $c1, $d1, 3;

    //---------------------------------------

    ncos_f32            $s1, $s0;
    nexp2_f32           $s1, $s0;
    nfma_f32            $s3, 1.0f, $s1, 23.0f;
    nfma_f64            $d3, 1.0L, $d0, $d3;
    nlog2_f32           $s1, $s0;
    nrcp_f32            $s1, $s0;
    nrsqrt_f32          $s1, $s0;
    nsin_f32            $s1, $s0;

    //---------------------------------------

    bitalign_b32        $s5, $s0, $s1, $s2;
    bytealign_b32       $s5, $s0, $s1, $s2;
    lerp_u8x4           $s5, $s0, $s1, $s2;
    
    packcvt_u8x4_f32    $s1, $s2, $s3, $s9, $s3;
    packcvt_u8x4_f32    $s1, 1.0f, 1.0f, 1.0f, 1.0f;
    
    unpackcvt_f32_u8x4  $s5, $s0, 0;
    unpackcvt_f32_u8x4  $s5, $s0, 1;
    unpackcvt_f32_u8x4  $s5, $s0, 2;
    unpackcvt_f32_u8x4  $s5, $s0, 3;
    
    sad_u32_u32         $s5, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF;

    sad_u32_u32         $s5, $s0, $s1, $s6;
    sad_u32_u16x2       $s5, $s0, $s1, $s6;
    sad_u32_u8x4        $s5, $s0, $s1, $s6;
    sadhi_u16x2_u8x4    $s5, $s0, $s1, $s6;

    //---------------------------------------

    segmentp_group_b1_u64 $c1, $d0;
    segmentp_group_b1_u64 $c1, 64;

    //---------------------------------------

    stof_spill_u64_u32      $d1, $s1;
    stof_private_u64_u32    $d1, $s1;
    stof_global_u64_u64     $d1, $d1;

    stof_spill_u64_u32      $d1, 64;
    stof_private_u64_u32    $d1, 64;
    stof_global_u64_u64     $d1, 64;

    ftos_group_u32_u64      $s1, $d2;
    ftos_global_u64_u64     $d1, $d2;

    ftos_group_u32_u64      $s1, 0;
    ftos_global_u64_u64     $d1, 0;

    //---------------------------------------

    cmp_eq_b1_b1            $c1, $c2, 0;
    cmp_eq_b32_b1           $s1, $c2, 0;
    cmp_eq_b32_b1           $s1, 1,   0;
    cmp_eq_f32_b1           $s1, $c2, 1;
    cmp_ne_b1_b1            $c1, $c2, 0;
    cmp_ne_b32_b1           $s1, $c2, 0;
    cmp_ne_f32_b1           $s1, $c2, 1;
    cmp_lt_b1_s32           $c1, $s2, 0;
    cmp_lt_b1_s32           $c1, -1,  0xFFFFFFFF;
    cmp_lt_b32_u32          $s1, $s2, 0xFFFFFFFF;
    cmp_lt_b32_u64          $s1, $d2, 0xFFFFFFFFFFFFFFFF;
    cmp_lt_f32_f32          $s1, $s2, 0.0f;
    cmp_lt_f32_f32          $s1, 1.0f, 0.0f;
    cmp_gt_b1_s32           $c1, $s2, 0;
    cmp_gt_b32_u32          $s1, $s2, 0;
    cmp_eq_f32_b32          $s1, $s2, 0.0f;
    cmp_equ_b1_f32          $c1, $s2, 0.f;
    cmp_equ_b1_f64          $c1, $d1, $d2;
    cmp_equ_b1_f64          $c1, 0.0, $d2;
    cmp_equ_b1_f64          $c1, $d1, 0.0;
    cmp_equ_b1_f64          $c1, 1.2, 1.1;
    cmp_sltu_b1_f32         $c1, $s2, 0.f;
    cmp_sltu_b1_f64         $c1, $d1, $d2;
    cmp_lt_pp_u8x4_u8x4     $s1, $s2, $s3;
    cmp_lt_pp_u16x2_f16x2   $s1, $s2, $s3;
    cmp_lt_ftz_pp_u16x2_f16x2   $s1, $s2, $s3;
    cmp_lt_pp_u32x2_f32x2   $d1, $d2, $d3;

    cmp_eq_b1_b1            $c1, $c2, $c0;
    cmp_eq_b32_b1           $s1, $c2, $c0;
    cmp_eq_b32_b1           $s1, $c2, $c0;
    cmp_eq_f32_b1           $s1, $c2, $c0;
    cmp_ne_b1_b1            $c1, $c2, $c0;
    cmp_ne_b32_b1           $s1, $c2, $c0;
    cmp_ne_f32_b1           $s1, $c2, $c0;
    cmp_lt_b1_u32           $c1, $s2, $s0;
    cmp_lt_b32_s32          $s1, $s2, $s0;
    cmp_lt_f32_f32          $s1, $s2, $s0;
    cmp_eq_b1_b32           $c1, $s2, $s0;
    cmp_gt_b32_u32          $s1, $s2, $s0;
    cmp_gt_f32_s32          $s1, $s2, $s0;
    cmp_equ_b1_f32          $c1, $s2, $s0;

    cmp_sltu_ftz_b32_f16    $s1, $s1, $s2;
    cmp_gt_f32_f32          $s1, $s2, $s0;
    cmp_sltu_b32_f64        $s1, $d1, $d2;

    cmp_lt_pp_u16x2_f16x2   $s1, $s2, $s3;
    cmp_lt_pp_u16x4_f16x4   $d1, $d2, $d3;
    cmp_lt_pp_u32x2_f32x2   $d1, $d2, $d3;
    cmp_lt_pp_u16x8_f16x8   $q1, $q2, $q3;
    cmp_lt_pp_u32x4_f32x4   $q1, $q2, $q3;
    cmp_lt_pp_u64x2_f64x2   $q1, $q2, $q3;

    cmp_lt_ftz_pp_u16x2_f16x2   $s1, $s2, $s3;
    cmp_lt_ftz_pp_u16x4_f16x4   $d1, $d2, $d3;
    cmp_lt_ftz_pp_u32x2_f32x2   $d1, $d2, $d3;
    cmp_lt_ftz_pp_u16x8_f16x8   $q1, $q2, $q3;
    cmp_lt_ftz_pp_u32x4_f32x4   $q1, $q2, $q3;
    cmp_lt_ftz_pp_u64x2_f64x2   $q1, $q2, $q3;

    cmp_lt_pp_u8x4_s8x4    $s1, $s2, $s3;
    cmp_lt_pp_u16x2_s16x2   $s1, $s2, $s3;
    cmp_lt_pp_u8x8_s8x8    $d1, $d2, $d3;
    cmp_lt_pp_u16x4_s16x4   $d1, $d2, $d3;
    cmp_lt_pp_u32x2_s32x2   $d1, $d2, $d3;
    cmp_lt_pp_u8x16_s8x16   $q1, $q2, $q3;
    cmp_lt_pp_u16x8_s16x8   $q1, $q2, $q3;
    cmp_lt_pp_u32x4_s32x4   $q1, $q2, $q3;
    cmp_lt_pp_u64x2_s64x2   $q1, $q2, $q3;

    //---------------------------------------

    // dest expansion
    cvt_b1_f32             $c1, 1.0f;
    cvt_b1_f32             $s1, $s2;
    cvt_b1_f32             $d1, $s2;

    cvt_u8_f32             $s1, $s2;
    cvt_u8_f32             $d1, $s2;

    cvt_s8_f32             $s1, $s2;
    cvt_s8_f32             $d1, $s2;

    cvt_u16_f32             $s1, $s2;
    cvt_u16_f32             $d1, $s2;

    cvt_s16_f32             $s1, $s2;
    cvt_s16_f32             $d1, $s2;

    cvt_u32_f32             $s1, $s2;
    cvt_u32_f32             $d1, $s2;

    cvt_s32_f32             $s1, $s2;
    cvt_s32_f32             $d1, $s2;

    // src expansion
    cvt_f32_b1              $s2, $c1;
    cvt_f32_b1              $s2, 1;
    cvt_f32_b1              $s2, $s1;
    cvt_f32_b1              $s2, $d1;

    cvt_f32_u8              $s2, $s1;
    cvt_f32_u8              $s2, $d1;

    cvt_f32_s8              $s2, $s1;
    cvt_f32_s8              $s2, -4;
    cvt_f32_s8              $s2, $d1;

    cvt_f32_u16             $s2, $s1;
    cvt_f32_u16             $s2, $d1;

    cvt_f32_s16             $s2, $s1;
    cvt_f32_s16             $s2, 123;
    cvt_f32_s16             $s2, $d1;

    cvt_f32_u32             $s2, 0xFFFFFFFF;
    cvt_f32_u64             $s2, 0xFFFFFFFFFFFFFFFF;
    cvt_f32_u32             $s2, $d1;

    cvt_f32_s32             $s2, $s1;
    cvt_f32_s32             $s2, -123;
    cvt_f32_s32             $s2, $d1;

    // common cases
    cvt_u32_f32             $s1, $s2;
    cvt_u32_f32             $d1, $s2;

    cvt_f32_f64             $s1, $d1;
    cvt_upi_u32_f32         $s1, $s2;
    cvt_ftz_f32_f32         $s1, $s2;
    cvt_u32_f32             $s1, $s2;
    cvt_f16_f32             $s1, $s2;
    cvt_s32_u8              $s1, $s2;
    cvt_s32_b1              $s1, $c2;
    cvt_s32_b1              $s1, $s2; // src may be wider
    cvt_s32_b1              $d1, $s2; // dst and src may be wider
    cvt_f32_f16             $s1, $s2;
    cvt_s32_f32             $s1, $s2;

    // f->s/u: int rounding
    cvt_ftz_upi_s8_f16      $s1, $s2;
    cvt_upi_sat_s8_f32      $s1, $s2;
    cvt_ftz_zeroi_s8_f64      $s1, $d2;
    cvt_zeroi_sat_s8_f64      $s1, $d2;
    cvt_ftz_downi_s8_f64      $s1, $d2;
    cvt_downi_sat_s8_f64      $s1, $d2;
    cvt_ftz_neari_s8_f64      $s1, $d2;
    cvt_neari_sat_s8_f64      $s1, $d2;

    cvt_ftz_upi_u8_f16      $s1, $s2;
    cvt_upi_sat_u8_f32      $s1, $s2;
    cvt_ftz_zeroi_u8_f64      $s1, $d2;
    cvt_zeroi_sat_u8_f64      $s1, $d2;
    cvt_ftz_downi_u8_f64      $s1, $d2;
    cvt_downi_sat_u8_f64      $s1, $d2;
    cvt_ftz_neari_u8_f64      $s1, $d2;
    cvt_neari_sat_u8_f64      $s1, $d2;

    // s/u->f: fp rounding
    cvt_near_f32_s32             $s2, $s1;
    cvt_up_f32_s32               $s2, $d1;
    cvt_down_f32_s32             $s2, $s1;
    cvt_zero_f32_s32             $s2, $d1;

    // f32->f64: no rounding, ftz supported
    cvt_ftz_f64_f32             $d2, $s1;

    // f32->f32: no rounding, ftz supported
    cvt_ftz_f32_f32             $s2, $s1;

    // f64->f32: fp rounding, ftz supported
    cvt_ftz_near_f32_f64        $s2, $d1;

    //---------------------------------------

    // dst expansion
    ld_global_s8                $s1, [&x][$d3+4];
    ld_global_s8                $d1, [&x][$d3+4];
    ld_global_u8                $s1, [&x][$d3+4];
    ld_global_u8                $d1, [&x][$d3+4];
    ld_global_s16               $s1, [&x][$d3+4];
    ld_global_s16               $d1, [&x];
    ld_global_u16               $s1, [&x];
    ld_global_u16               $d1, [&x];
    ld_global_s32               $d1, [&x];
    ld_global_u32               $d1, [&x];

    ld_v2_s8                   ($s1,$s2), [$d3+4];
    ld_v2_s8                   ($d1,$d2), [$d3+4];
    ld_v2_u8                   ($s1,$s2), [$d3+4];
    ld_v2_u8                   ($d1,$d2), [$d3+4];
    ld_v2_s16                  ($s1,$s2), [$d3+4];
    ld_v2_s16                  ($d1,$d2), [$d3+4];
    ld_v2_u16                  ($s1,$s2), [$d3+4];
    ld_v2_u16                  ($d1,$d2), [$d3+4];
    ld_v2_s32                  ($s1,$s2), [$d3+4];
    ld_v2_s32                  ($d1,$d2), [$d3+4];
    ld_v2_u32                  ($s1,$s2), [$d3+4];
    ld_v2_u32                  ($d1,$d2), [$d3+4];

    ld_v4_s8                   ($s1,$s2,$s3,$s4), [$d3+4];
    ld_v4_s8                   ($d1,$d2,$d3,$d4), [$d3+4];
    ld_v4_u8                   ($s1,$s2,$s3,$s4), [$d3+4];
    ld_v4_u8                   ($d1,$d2,$d3,$d4), [$d3+4];
    ld_v4_s16                  ($s1,$s2,$s3,$s4), [$d3+4];
    ld_v4_s16                  ($d1,$d2,$d3,$d4), [$d3+4];
    ld_v4_u16                  ($s1,$s2,$s3,$s4), [$d3+4];
    ld_v4_u16                  ($d1,$d2,$d3,$d4), [$d3+4];
    ld_v4_s32                  ($s1,$s2,$s3,$s4), [$d3+4];
    ld_v4_s32                  ($d1,$d2,$d3,$d4), [$d3+4];
    ld_v4_u32                  ($s1,$s2,$s3,$s4), [$d3+4];
    ld_v4_u32                  ($d1,$d2,$d3,$d4), [$d3+4];

    // segment rules
    ld_global_f32               $s1, [&x][$d0];
    ld_arg_acq_equiv(2)_f32     $s1, [%arg][$s0];
    ld_group_equiv(0)_u32       $s0, [&g][$s0];

    // b128
    ld_global_b128              $q1, [$d0];

    // width
    ld_width(64)_global_f16     $s1, [$d0];
    ld_width(all)_f16           $s1, [$d0];
    ld_width(WAVESIZE)_f16      $s1, [$d0];

    // opaque refs
    ld_global_roimg               $d1, [&roimage];
    ld_global_rwimg               $d1, [&rwimage];
    ld_global_samp                $d1, [&samp];

/*    ld_global_roimg               $d1, [$d0];
    ld_global_rwimg               $d1, [$d0];
    ld_global_samp                $d1, [$d0]; */

    global_b8   %b8var;
    ld_global_f32                $s1, [%b8var];

    // generic cases
    ld_global_s32               $s1, [&x];
    ld_global_f16               $s1, [&x];
    ld_global_f64               $d1, [&x];
    ld_global_aligned_f64       $d1, [&x];
    ld_global_acq_f32           $s1, [&x];
    ld_global_acq_f64           $d1, [&x];
    ld_global_acq_equiv(2)_f32  $s1, [&x];
    ld_global_acq_equiv(2)_f32  $s1, [$d3+4];

    ld_private_f32              $s1, [$s3+4];
    ld_spill_f32                $s1, [$s3+4];
    ld_f32                      $s1, [$d3+4];
    ld_aligned_f32              $s1, [$d3+4];
    ld_v3_s32                   ($s1,$s2,$s6), [$d3+4];
    ld_v4_f32                   ($s1,$s7,$s6,$s2), [$d3+4];
    ld_v2_equiv(9)_f32          ($s1,$s2), [$d3+4];
    ld_equiv(1)_u64             $d3, [$d4+32];
    ld_v2_equiv(1)_u64          ($d1,$d2), [$d0+32];
    ld_v4_width(8)_f32          ($s1,$s3,$s6,$s2), [$d3+4];
    ld_equiv(1)_u64             $d6, [128];
    ld_v2_width(4)_equiv(9)_f32 ($s1,$s2), [$d3+4];
    ld_width(64)_u32            $s0, [$d2];
    ld_width(1024)_equiv(1)_u64 $d6, [128];
    ld_width(all)_equiv(1)_u64  $d6, [128];

    ld_v2_equiv(0)_u64          ($d1,$d2), [$d0+32];
    ld_v2_equiv(1)_u64          ($d1,$d2), [$d0+32];
    ld_v2_equiv(254)_u64          ($d1,$d2), [$d0+32];
    ld_v2_equiv(255)_u64          ($d1,$d2), [$d0+32];

    ld_width(1)_u64 $d6, [128];
    ld_width(2)_u64 $d6, [128];
    ld_width(4)_u64 $d6, [128];
    ld_width(8)_u64 $d6, [128];
    ld_width(16)_u64 $d6, [128];
    ld_width(32)_u64 $d6, [128];
    ld_width(64)_u64 $d6, [128];
    ld_width(128)_u64 $d6, [128];
    ld_width(256)_u64 $d6, [128];
    ld_width(512)_u64 $d6, [128];
    ld_width(1024)_u64 $d6, [128];
    ld_width(2048)_u64 $d6, [128];
    ld_width(4096)_u64 $d6, [128];
    ld_width(8192)_u64 $d6, [128];
    ld_width(16384)_u64 $d6, [128];
    ld_width(32768)_u64 $d6, [128];
    ld_width(65536)_u64 $d6, [128];
    ld_width(131072)_u64 $d6, [128];
    ld_width(262144)_u64 $d6, [128];
    ld_width(524288)_u64 $d6, [128];
    ld_width(1048576)_u64 $d6, [128];
    ld_width(2097152)_u64 $d6, [128];
    ld_width(4194304)_u64 $d6, [128];
    ld_width(8388608)_u64 $d6, [128];
    ld_width(16777216)_u64 $d6, [128];
    ld_width(33554432)_u64 $d6, [128];
    ld_width(67108864)_u64 $d6, [128];
    ld_width(134217728)_u64 $d6, [128];
    ld_width(268435456)_u64 $d6, [128];
    ld_width(536870912)_u64 $d6, [128];
    ld_width(1073741824)_u64 $d6, [128];
    ld_width(2147483648)_u64 $d6, [128];
    ld_width(WAVESIZE)_u64 $d6, [128];
    ld_width(all)_u64 $d6, [128];

    ld_width(0x2)_u64 $d6, [128];
    ld_width(0x10)_u64 $d6, [128];

    //---------------------------------------

    // src expansion
    st_global_s8                $s1, [&x];
    st_global_s8                $d1, [&x];
    st_global_u8                $s1, [&x];
    st_global_u8                $d1, [&x];
    st_global_s16               $s1, [&x];
    st_global_s16               $d1, [&x];
    st_global_u16               $s1, [&x];
    st_global_u16               $d1, [&x];
    st_global_s32               $d1, [&x];
    st_global_u32               $d1, [&x];

    st_v2_s8                   ($s1,$s2), [$d3+4];
    st_v2_s8                   ($d1,$d2), [$d3+4];
    st_v2_u8                   ($s1,$s2), [$d3+4];
    st_v2_u8                   ($d1,$d2), [$d3+4];
    st_v2_s16                  ($s1,$s2), [$d3+4];
    st_v2_s16                  ($d1,$d2), [$d3+4];
    st_v2_u16                  ($s1,$s2), [$d3+4];
    st_v2_u16                  ($d1,$d2), [$d3+4];
    st_v2_s32                  ($s1,$s2), [$d3+4];
    st_v2_s32                  ($d1,$d2), [$d3+4];
    st_v2_u32                  ($s1,$s2), [$d3+4];
    st_v2_u32                  ($d1,$d2), [$d3+4];

    st_v4_s8                   ($s1,$s2,$s3,$s4), [$d3+4];
    st_v4_s8                   ($d1,$d2,$d3,$d4), [$d3+4];
    st_v4_u8                   ($s1,$s2,$s3,$s4), [$d3+4];
    st_v4_u8                   ($d1,$d2,$d3,$d4), [$d3+4];
    st_v4_s16                  ($s1,$s2,$s3,$s4), [$d3+4];
    st_v4_s16                  ($d1,$d2,$d3,$d4), [$d3+4];
    st_v4_u16                  ($s1,$s2,$s3,$s4), [$d3+4];
    st_v4_u16                  ($d1,$d2,$d3,$d4), [$d3+4];
    st_v4_s32                  ($s1,$s2,$s3,$s4), [$d3+4];
    st_v4_s32                  ($d1,$d2,$d3,$d4), [$d3+4];
    st_v4_u32                  ($s1,$s2,$s3,$s4), [$d3+4];
    st_v4_u32                  ($d1,$d2,$d3,$d4), [$d3+4];

    // segment rules
    st_global_f32               $s1, [&x][$d0];
    st_arg_equiv(2)_f32         $s1, [%arg][$s0];
    st_group_equiv(0)_u32       $s0, [&g][$s0];

    // b128
    st_global_b128              $q1, [$d0];

    // opaque refs
    st_global_roimg            $d1, [&roimage];
    st_global_rwimg            $d1, [&rwimage];
    st_global_samp             $d1, [&samp];

    st_global_f32              $s1, [&x];
    st_global_s32              $d1, [&x]; // src of 'st' may be wider than type size
    st_global_aligned_f32      $s1, [&x];
    st_global_u8               $s1, [&x];
    st_global_u16              $s1, [&x];
    st_global_u32              $s1, [&x];
    st_global_f16              $s1, [&x];
    st_global_f64              $d1, [&x];
    st_global_aligned_f64      $d1, [&x];
    st_global_rel_f32          $s1, [&x];
    st_global_rel_f64          $d1, [&x];
    st_global_rel_equiv(2)_f32 $s1, [&x];

    st_rel_equiv(2)_f32      $s1, [$d3+4];
    st_private_f32           $s1, [$s3+4];
    st_global_f32            $s1, [$d3+4];
    st_spill_f32             $s1, [$s3+4];
    st_arg_f32               $s1, [$s3+4];
    st_f32                   $s1, [$d3+4];
    st_aligned_f32           $s1, [$d3+4];
    st_v4_f32                ($s1,$s1,$s6,$s2), [$d3+4];
    st_v2_equiv(9)_f32       ($s1,$s2),         [$d3+4];
    st_v3_s32                ($s1,$s1,$s6),     [$d3+4];
    st_group_equiv(0)_u32    $s0,               [$s2];
    st_equiv(1)_u64          $d3,               [$d4+32];
    st_aligned_equiv(1)_u64  $d3,               [$d4+32];
    st_v2_equiv(1)_u64       ($d1,$d2),         [$d0+32];
    st_equiv(1)_u64          $d6,               [128];

    //---------------------------------------

    atomic_and_global_ar_b32    $s1, [&x][$d0], 23;
    atomic_and_global_b32       $s1, [&x], 23;
    atomic_or_global_ar_b64     $d1, [&x], 23;
    atomic_or_global_b64        $d1, [&x], 23;
    atomic_xor_global_ar_b64    $d1, [&x], 23;
    atomic_xor_global_b64       $d1, [&x], 23;
    atomic_cas_global_ar_b64    $d1, [&x], 23, 12;
    atomic_cas_global_b64       $d1, [&x], 23, 1;
    atomic_exch_global_ar_b64   $d1, [&x], 23;
    atomic_exch_global_b64      $d1, [&x], 23;
    atomic_add_global_ar_u64    $d1, [&x], 23;
    atomic_add_global_s64       $d1, [&x], 23;
    atomic_sub_global_ar_u64    $d1, [&x], 23;
    atomic_sub_global_s64       $d1, [&x], 23;
    atomic_inc_global_ar_u64    $d1, [&x], 23;
    atomic_inc_global_u64       $d1, [&x], 23;
    atomic_dec_global_ar_u64    $d1, [&x], 23;
    atomic_dec_global_u64       $d1, [&x], 23;
    atomic_max_global_ar_s64    $d1, [&x], 23;
    atomic_max_global_u64       $d1, [&x], 23;
    atomic_min_global_ar_s64    $d1, [&x], 23;
    atomic_min_global_u64       $d1, [&x], 23;
    atomic_and_global_ar_b32    $s1, [&x], 23;
    atomic_and_global_b32       $s1, [&x], 23;
    atomic_or_global_ar_b64     $d1, [&x], 23;
    atomic_or_global_b64        $d1, [&x], 23;
    atomic_xor_global_ar_b64    $d1, [&x], 23;
    atomic_xor_global_b64       $d1, [&x], 23;
    atomic_cas_global_ar_b64    $d1, [&x], 23, 12;
    atomic_cas_global_b64       $d1, [&x], 23, 1;
    atomic_exch_global_ar_b64   $d1, [&x], 23;
    atomic_exch_global_b64      $d1, [&x], 23;
    atomic_add_global_ar_u64    $d1, [&x], 23;
    atomic_add_global_s64       $d1, [&x], 23;
    atomic_sub_global_ar_u64    $d1, [&x], 23;
    atomic_sub_global_s64       $d1, [&x], 23;
    atomic_inc_global_ar_u64    $d1, [&x], 23;
    atomic_inc_global_u64       $d1, [&x], 23;
    atomic_dec_global_ar_u64    $d1, [&x], 23;
    atomic_dec_global_u64       $d1, [&x], 23;
    atomic_max_global_ar_s64    $d1, [&x], 23;
    atomic_max_global_u64       $d1, [&x], 23;
    atomic_min_global_ar_s64    $d1, [&x], 23;
    atomic_min_global_u64       $d1, [&x], 23;

    atomic_and_group_b32    $s1, [&g][$s0], 23;
    atomic_or_group_b64     $d1, [&g], 23;
    atomic_xor_group_b64    $d1, [&g], 23;
    atomic_cas_group_b64    $d1, [&g], 23, 9;
    atomic_exch_group_b64   $d1, [&g], 23;
    atomic_add_group_u64    $d1, [&g], 23;
    atomic_sub_group_u64    $d1, [&g], 23;
    atomic_inc_group_u64    $d1, [&g], 23;
    atomic_dec_group_u64    $d1, [&g], 23;
    atomic_max_group_u64    $d1, [&g], 23;
    atomic_min_group_u64    $d1, [&g], 23;
    atomic_and_group_b32    $s1, [&g], 23;
    atomic_or_group_b64     $d1, [&g], 23;
    atomic_xor_group_b64    $d1, [&g], 23;
    atomic_cas_group_b64    $d1, [&g], 23, 9;
    atomic_exch_group_b64   $d1, [&g], 23;
    atomic_add_group_u64    $d1, [&g], 23;
    atomic_sub_group_u64    $d1, [&g], 23;
    atomic_inc_group_u64    $d1, [&g], 23;
    atomic_dec_group_u64    $d1, [&g], 23;
    atomic_max_group_s64    $d1, [&g], 23;
    atomic_min_group_s64    $d1, [&g], 23;

    atomic_and_b32          $s1, [$d2], 23;
    atomic_or_b64           $d1, [$d4], 23;
    atomic_xor_b64          $d1, [$d3], 23;
    atomic_cas_b64          $d1, [$d5], 23, 12;
    atomic_exch_b64         $d1, [$d4], 23;
    atomic_add_u64          $d1, [$d6], 23;
    atomic_sub_u64          $d1, [$d3], 23;
    atomic_inc_u64          $d1, [$d3], 23;
    atomic_dec_u64          $d1, [$d4], 23;
    atomic_max_u64          $d1, [$d5], 23;
    atomic_and_b32          $s1, [$d2], 23;
    atomic_or_b64           $d1, [$d4], 23;
    atomic_xor_b64          $d1, [$d3], 23;
    atomic_cas_b64          $d1, [$d5], 23, 12;
    atomic_exch_b64         $d1, [$d4], 23;
    atomic_add_s64          $d1, [$d6], 23;
    atomic_sub_s64          $d1, [$d3], 23;
    atomic_inc_u64          $d1, [$d3], 23;
    atomic_dec_u64          $d1, [$d4], 23;
    atomic_max_u64          $d1, [$d5], 23;
    atomic_min_u64          $d1, [$d7], 23;
    atomic_min_u64          $d1, [$d7], 23;

    //---------------------------------------

    atomicnoret_and_global_ar_b32   [&x], 23;
    atomicnoret_and_global_b32      [&x], 23;
    atomicnoret_or_global_ar_b64    [&x], 23;
    atomicnoret_or_global_b64       [&x], 23;
    atomicnoret_xor_global_ar_b64   [&x], 23;
    atomicnoret_xor_global_b64      [&x], 23;
    atomicnoret_cas_global_ar_b64   [&x], 23, 12;
    atomicnoret_cas_global_b64      [&x], 23, 1;
    atomicnoret_add_global_ar_u64   [&x], 23;
    atomicnoret_add_global_s64      [&x], 23;
    atomicnoret_sub_global_ar_u64   [&x], 23;
    atomicnoret_sub_global_s64      [&x], 23;
    atomicnoret_inc_global_ar_u64   [&x], 23;
    atomicnoret_inc_global_u64      [&x], 23;
    atomicnoret_dec_global_ar_u64   [&x], 23;
    atomicnoret_dec_global_u64      [&x], 23;
    atomicnoret_max_global_ar_u64   [&x], 23;
    atomicnoret_max_global_s64      [&x], 23;
    atomicnoret_min_global_ar_u64   [&x], 23;
    atomicnoret_min_global_s64      [&x], 23;

    atomicnoret_and_group_b32   [&g][4], 23;
    atomicnoret_or_group_b64    [&g][$s0], 23;
    atomicnoret_xor_group_b64   [&g], 23;
    atomicnoret_cas_group_b64   [&g], 23, 9;
    atomicnoret_add_group_u64   [&g], 23;
    atomicnoret_sub_group_u64   [&g], 23;
    atomicnoret_inc_group_u64   [&g], 23;
    atomicnoret_dec_group_u64   [&g], 23;
    atomicnoret_max_group_u64   [&g], 23;
    atomicnoret_min_group_u64   [&g], 23;

    atomicnoret_and_global_b32   [&x][4], 23;
    atomicnoret_or_global_b64    [&x][$d0], 23;
    atomicnoret_xor_global_b64   [&x], 23;
    atomicnoret_cas_global_b64   [&x], 23, 9;
    atomicnoret_add_global_u64   [&x], 23;
    atomicnoret_sub_global_u64   [&x], 23;
    atomicnoret_inc_global_u64   [&x], 23;
    atomicnoret_dec_global_u64   [&x], 23;
    atomicnoret_max_global_u64   [&x], 23;
    atomicnoret_min_global_u64   [&x], 23;

    atomicnoret_and_b32     [$d1], 23;
    atomicnoret_or_b64      [$d2], 23;
    atomicnoret_xor_b64     [$d3], 23;
    atomicnoret_cas_b64     [$d2], 23, 12;
    atomicnoret_add_s64     [$d4], 23;
    atomicnoret_sub_s64     [$d5], 23;
    atomicnoret_inc_u64     [$d2], 23;
    atomicnoret_dec_u64     [$d6], 23;
    atomicnoret_max_s64     [$d3], 23;
    atomicnoret_min_s64     [$d4], 23;

    //---------------------------------------

    rdimage_v4_1d_s32_rwimg_f32     ($s0, $s1, $s5, $s3), $d1, $d3,    $s6;
    rdimage_v4_1da_u32_rwimg_u32    ($s0, $s1, $s2, $s3), $d1, $d3,    ($s6, $s7);
    rdimage_v4_2d_s32_roimg_u32     ($s0, $s1, $s3, $s4), $d2, $d3,    ($s6, $s9);
    rdimage_v4_2da_f32_rwimg_f32    ($s0, $s1, $s3, $s4), $d1, $d3,    ($s6, $s9, $s12);
    rdimage_v4_3d_s32_roimg_f32     ($s0, $s1, $s3, $s4), $d2, $d3,    ($s6, $s9, $s2);

    //---------------------------------------

    ldimage_v4_1d_u32_roimg_u32     ($s1, $s2, $s3, $s4), $d2,    $s4;
    ldimage_v4_1db_u32_roimg_u32    ($s1, $s2, $s3, $s4), $d2,    $s4;
    ldimage_v4_2d_s32_rwimg_u32     ($s1, $s2, $s3, $s4), $d1,    ($s4, $s5);
    ldimage_v4_1da_s32_rwimg_u32    ($s1, $s2, $s3, $s4), $d1,    ($s4, $s5);
    ldimage_v4_3d_f32_rwimg_u32     ($s1, $s2, $s3, $s4), $d1,    ($s4, $s5, $s6);
    ldimage_v4_2da_f32_roimg_u32    ($s1, $s2, $s3, $s4), $d2,    ($s4, $s1, $s2);

    //---------------------------------------

    stimage_v4_1d_f32_rwimg_u32     ($s1, $s2, $s3, $s4), $d1,    $s4;
    stimage_v4_1db_f32_rwimg_u32    ($s1, $s2, $s3, $s4), $d1,    $s4;
    stimage_v4_2d_u32_rwimg_u32     ($s1, $s2, $s3, $s4), $d1,    ($s4, $s5);
    stimage_v4_1da_u32_rwimg_u32    ($s1, $s2, $s3, $s4), $d1,    ($s4, $s5);
    stimage_v4_3d_f32_rwimg_u32     ($s1, $s2, $s3, $s4), $d1,    ($s4, $s5, $s6);
    stimage_v4_2da_s32_rwimg_u32    ($s1, $s2, $s3, $s4), $d1,    ($s4, $s5, $s6);

    //---------------------------------------

    atomicimage_and_1d_b32_rwimg_u32        $s3,    $d1,    $s1,              $s10;
    atomicimage_or_1d_b32_rwimg_u32         $s3,    $d1,    $s1,              $s3;
    atomicimage_xor_1db_b32_rwimg_u32       $s0,    $d1,    $s1,              $s3;
    atomicimage_cas_1d_b32_rwimg_u32        $s10,   $d1,    $s1,              $s3, $s4;
    atomicimage_min_1d_s64_rwimg_u32        $d3,    $d2,    $s1,              $d4;
    atomicimage_min_1d_u64_rwimg_u32        $d3,    $d2,    $s1,              $d4;
    atomicimage_max_1d_s64_rwimg_u32        $d3,    $d2,    $s1,              $d4;
    atomicimage_max_1d_u64_rwimg_u32        $d3,    $d2,    $s1,              $d4;

    atomicimage_and_2d_b32_rwimg_u32        $s2,    $d1,    ($s0, $s3),       $s2;
    atomicimage_sub_2d_s64_rwimg_u32        $d3,    $d2,    ($s0, $s3),       $d4;
    atomicimage_sub_2d_u64_rwimg_u32        $d3,    $d2,    ($s0, $s3),       $d4;
    atomicimage_add_2d_s64_rwimg_u32        $d3,    $d2,    ($s0, $s3),       $d4;
    atomicimage_add_2d_u64_rwimg_u32        $d3,    $d2,    ($s0, $s3),       $d4;
    atomicimage_max_1da_u64_rwimg_u32       $d3,    $d2,    ($s1, $s2),       $d4;
    atomicimage_inc_2d_u64_rwimg_u32        $d3,    $d2,    ($s0, $s3),       $d4;
    atomicimage_dec_2d_u64_rwimg_u32        $d3,    $d2,    ($s0, $s3),       $d4;

    atomicimage_and_3d_b32_rwimg_u32        $s1,    $d1,    ($s0, $s3, $s1),  $s1;
    atomicimage_add_3d_s32_rwimg_u32        $s4,    $d1,    ($s0, $s3, $s1),  $s2;
    atomicimage_exch_2da_b64_rwimg_u32      $d3,    $d2,    ($s1, $s2, $s3),  $d4;

    //---------------------------------------

    atomicimagenoret_and_1d_b32_rwimg_u32       $d1, $s1,               $s10;
    atomicimagenoret_and_1d_b64_rwimg_u32       $d1, $s1,               $d10;
    atomicimagenoret_or_1d_b32_rwimg_u32        $d1, $s1,               $s3;
    atomicimagenoret_or_1d_b64_rwimg_u32        $d1, $s1,               $d3;
    atomicimagenoret_xor_1db_b32_rwimg_u32      $d1, $s1,               $s3;
    atomicimagenoret_xor_1db_b64_rwimg_u32      $d1, $s1,               $d3;
    atomicimagenoret_cas_1d_b32_rwimg_u32       $d1, $s1,               $s3, $s4;
    atomicimagenoret_cas_1d_b64_rwimg_u32       $d1, $s1,               $d3, $d4;

    atomicimagenoret_min_1d_u64_rwimg_u32       $d2, $s1,               $d4;
    atomicimagenoret_min_1d_s64_rwimg_u32       $d2, $s1,               $d4;
    atomicimagenoret_min_1d_u32_rwimg_u32       $d2, $s1,               $s4;
    atomicimagenoret_min_1d_s32_rwimg_u32       $d2, $s1,               $s4;
    atomicimagenoret_max_1d_u64_rwimg_u32       $d2, $s1,               $d4;
    atomicimagenoret_max_1d_s64_rwimg_u32       $d2, $s1,               $d4;
    atomicimagenoret_max_1d_u32_rwimg_u32       $d2, $s1,               $s4;
    atomicimagenoret_max_1d_s32_rwimg_u32       $d2, $s1,               $s4;
    atomicimagenoret_add_1d_u64_rwimg_u32       $d2, $s1,               $d4;
    atomicimagenoret_add_1d_s64_rwimg_u32       $d2, $s1,               $d4;
    atomicimagenoret_add_1d_u32_rwimg_u32       $d2, $s1,               $s4;
    atomicimagenoret_add_1d_s32_rwimg_u32       $d2, $s1,               $s4;
    atomicimagenoret_sub_1d_u64_rwimg_u32       $d2, $s1,               $d4;
    atomicimagenoret_sub_1d_s64_rwimg_u32       $d2, $s1,               $d4;
    atomicimagenoret_sub_1d_u32_rwimg_u32       $d2, $s1,               $s4;
    atomicimagenoret_sub_1d_s32_rwimg_u32       $d2, $s1,               $s4;

    atomicimagenoret_inc_1d_u64_rwimg_u32       $d2, $s1,               $d4;
    atomicimagenoret_inc_1d_u32_rwimg_u32       $d2, $s1,               $s4;
    atomicimagenoret_dec_1d_u64_rwimg_u32       $d2, $s1,               $d4;
    atomicimagenoret_dec_1d_u32_rwimg_u32       $d2, $s1,               $s4;

    atomicimagenoret_and_2d_b32_rwimg_u32       $d1, ($s0, $s3),        $s2;
    atomicimagenoret_sub_2d_s64_rwimg_u32       $d2, ($s0, $s3),        $d4;
    atomicimagenoret_max_1da_u64_rwimg_u32      $d2, ($s1, $s2),        $d4;

    atomicimagenoret_and_3d_b32_rwimg_u32       $d1, ($s0, $s3, $s1),   $s1;
    atomicimagenoret_add_3d_s32_rwimg_u32       $d1, ($s0, $s3, $s1),   $s2;

    //---------------------------------------

    queryimagewidth_u32_rwimg        $s1, $d1;
    queryimageheight_u32_rwimg       $s0, $d1;
    queryimagedepth_u32_rwimg        $s0, $d1;
    queryimagearray_u32_roimg        $s1, $d2;
    queryimageorder_u32_roimg        $s0, $d2;
    queryimageformat_u32_roimg       $s0, $d2;
    querysamplercoord_u32_samp       $s0, $d3;
    querysamplerfilter_u32_samp      $s0, $d3;

    //---------------------------------------

   cbr_width(2)    $c0, @label1;
   cbr_width(4)    $c0, @label1;
   cbr_width(8)    $c0, @label1;
   cbr_width(16)    $c0, @label1;
   cbr_width(32)    $c0, @label1;
   cbr_width(64)    $c0, @label1;
   cbr_width(128)    $c0, @label1;
   cbr_width(256)    $c0, @label1;
   cbr_width(512)    $c0, @label1;
   cbr_width(1024)    $c0, @label1;
   cbr_width(2048)    $c0, @label1;
   cbr_width(4096)    $c0, @label1;
   cbr_width(8192)    $c0, @label1;
   cbr_width(16384)    $c0, @label1;
   cbr_width(32768)    $c0, @label1;
   cbr_width(65536)    $c0, @label1;
   cbr_width(131072)    $c0, @label1;
   cbr_width(262144)    $c0, @label1;
   cbr_width(524288)    $c0, @label1;
   cbr_width(1048576)    $c0, @label1;
   cbr_width(2097152)    $c0, @label1;
   cbr_width(4194304)    $c0, @label1;
   cbr_width(8388608)    $c0, @label1;
   cbr_width(16777216)    $c0, @label1;
   cbr_width(33554432)    $c0, @label1;
   cbr_width(67108864)    $c0, @label1;
   cbr_width(134217728)    $c0, @label1;
   cbr_width(268435456)    $c0, @label1;
   cbr_width(536870912)    $c0, @label1;
   cbr_width(1073741824)    $c0, @label1;
   cbr_width(2147483648)    $c0, @label1;
   cbr_width(all)          $c0, @label2;
   cbr_width(WAVESIZE)     $c0, @label2;
   cbr                     $c0, @label3;
   brn             @label2;

@label10:
   brn_width(all)          @label2;

@label:
    brn $d1, [@targets];

@label9:
    cbr            $c1, $d1;
    cbr_width(2)   $c1, $d1;
    cbr_width(all) $c1, $d1;

    global_u64 %jumptable[3]  = {@label, @label2, @label3};
    global_u64 %jumptable2[3] = {@label, @label2, @label3};

    cbr             $c1, $d1, [@targets];
    cbr_width(2)    $c1, $d1, [%jumptable];
    cbr_width(all)  $c1, $d1, [@targets];

    @targets: labeltargets @label1, @label2, @label3;

///    brn $d1, [@targets];

@label1:
    abs_s32 $s1, $s2;

@label2:
    brn $d1, [%jumptable2];

@label3:
    abs_s32 $s1, $s2;

    //---------------------------------------

    //---------------------------------------

    barrier;
    barrier_width(64);
    barrier_fgroup;
    barrier_fglobal;

    //---------------------------------------

///    { fbarrier            %fb0; }// fbarrier declaration
///
///    initfbar            %fb0;
///    joinfbar            %fb0;
///    waitfbar_fglobal    %fb0;
///    waitfbar            %fb0;
///    arrivefbar_fpartial %fb0;
///    leavefbar           %fb0;
///    releasefbar         %fb0;
///    ldf_u32             $s0, %fb0;
///    joinfbar            $s0;

    fbarrier            %fb;

    initfbar            %fb;
    joinfbar            %fb;
    waitfbar            %fb;
    leavefbar           %fb;
    releasefbar         %fb;
    ldf_u32             $s0, %fb;

    initfbar            $s0;
    joinfbar            $s0;
    waitfbar            $s0;
    leavefbar           $s0;
    releasefbar         $s0;
    arrivefbar          $s0;

    waitfbar_fgroup          %fb;
    waitfbar_fglobal         %fb;
    waitfbar_fboth           %fb;
    waitfbar_fpartial        %fb;
    waitfbar_fpartialboth    %fb;

    arrivefbar_fgroup          %fb;
    arrivefbar_fglobal         %fb;
    arrivefbar_fboth           %fb;
    arrivefbar_fpartial        %fb;
    arrivefbar_fpartialboth    %fb;

    joinfbar_width(1)          %fb;
    leavefbar_width(1)         %fb;
    waitfbar_width(1)          %fb;
    arrivefbar_width(1)        %fb;

    //---------------------------------------

    sync;
    sync_fgroup;
    sync_fglobal;
    sync_fboth;
    sync_fpartial;
    sync_fpartialboth;

    //---------------------------------------

    countlane_u32               $s1, $s2;
    countlane_u32               $s1, 1;
    masklane_b64                $d1, $c0;
    masklane_b64                $d1, $s0;
    masklane_b64                $d1, 12345;
    masklane_b64                $d1, 0xFFFFFFFF;
    masklane_b64                $d1, 1;

    countuplane_u32             $s1;
    sendlane_b32                $s1, $s2, 3;
    sendlane_b32                $s1, 3, $s2;
    sendlane_b32                $s1, $s2, $s2;
    receivelane_b32             $s1, $s2, 23;
    receivelane_b32             $s1, 23, $s2;

    //---------------------------------------

    syscall_u32                 $s1, 3, $s2, $s3, $s4;
    syscall_u64                 $d1, 10, $d2, 100, $d4;

    //---------------------------------------

    alloca_private_u32          $s1, $s0;
    alloca_private_u32          $s1, 24;

    //---------------------------------------

    dispatchptr_global_u64      $d0; 
    qptr_global_u64             $d0;    
    nullptr_group_u32           $s0;    
    nullptr_global_u64          $d1;    
    nullptr_flat_u64            $d1;    

    //---------------------------------------

    clock_u64                   $d6; 
//F    debugtrap_u32               $s1; 
    cuid_u32                    $s7; 
    maxcuid_u32                 $s6; 
    waveid_u32                  $s3; 
    maxwaveid_u32               $s4; 
    dispatchid_u64              $d0; 
    laneid_u32                  $s1; 
    qid_u32                     $s0; 
//F    cleardetectexcept_u32       $s0;
//F    getdetectexcept_u32         $s1;
//F    setdetectexcept_u32         $s2;
//    nop;                                
    gridsize_u32                $s2, 2; 
    gridgroups_u32              $s2, 2; 
    workgroupsize_u32           $s1, 0; 
    currentworkgroupsize_u32    $s1, 0; 
    currentworkgroupsize_u32    $s1, 1; 
    currentworkgroupsize_u32    $s1, 2; 
    workitemabsid_u32           $s1, 0; 
    workgroupid_u32             $s1, 0; 
    workgroupid_u32             $s1, 1; 
    workgroupid_u32             $s1, 2; 
    workitemid_u32              $s1, 0; 
    workitemid_u32              $s1, 1; 
    workitemid_u32              $s1, 2; 

};

function &TestInst2()(arg_u32 %arg)
{
@targets: labeltargets @label1, @label2, @label3;

@label1:
@label2:
    brn $d1, [@targets];
@label3:
    ret;
};

function &TestInst3()(arg_u32 %arg)
{

@label:


@label1:
@label2:
    brn $d1, [@targets];
@label3:
    ret;

@targets: labeltargets @label1, @label2, @label3;

};
